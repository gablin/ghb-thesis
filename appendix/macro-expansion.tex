% Copyright (c) 2018, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons 4.0 International License (see
% LICENSE file or visit <http://creativecommons.org/licenses/by/4.0/> for a copy
% of the license).

\chapter{Macro Expansion}
\labelAppendix{macro-expansion}

This appendix considers techniques based on \gls{macro expansion}.
%
First, we introduce the principle in \refSection{me-principle}.
%
We then describe early applications in \refSection{me-naive-macro-expansion},
moving on to more sophisticated techniques in
\refSection{me-peephole-optimization}.
%
We discuss limitations of this principle in \refSection{me-limitations} and then
summarize in \refSection{me-summary}.

The appendix is based on material presented in
\cite[Chap.\thinspace2]{HjortBlindell:2016:Survey} that has been adapted for
this dissertation.
%
To not disturb the flow of reading, material already presented in
\refChapter{existing-isel-techniques-and-reps} is duplicated in this appendix.
%
The techniques described here includes all those covered in earlier surveys by
\textcite{Cattell:1977} and \textcite{GanapathiEtAl:1982:Survey}, several of
which are also discussed in depth by \textcite{Lunell:1983}.
%
In \cite{GanapathiEtAl:1982:Survey} this \gls{principle} is called
\gls!{interpretative.cg}[ \gls{code generation}].


\section{The Principle}
\labelSection{me-principle}

The first \gls{principle} to emerge was \gls{macro expansion}, with applications
starting to appear in the 1960s.
%
In \gls{macro expansion}, the \glspl{instruction} are expressed as
\glspl!{macro} which consist of two parts:
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item a \gls!{template} to be matched over the \gls{function}
  \item an \gls!{expand procedure} to be executed upon the part of the
    \gls{function} that was matched (see \refFigure{macro-example-2} for an
    example)
\end{inlinelist}.
%
\begin{filecontents*}{macro-example-2.c}
expand($\irAssign{\text{\$3}}{\irAdd{\text{\$1}}{\text{\$2}}}$) {
  r1 = getRegOf($\$$1);
  r2 = getRegOf($\$$2);
  r3 = mkNewReg($\$$3);
  print "add " + r3 + ", " + r1 + ", " + r2;
}
\end{filecontents*}%
%
\begin{figure}
  \centering%
  \begin{minipage}{.6\textwidth}
    \lstinputlisting[mathescape]{macro-example-2.c}
  \end{minipage}

  \caption[Example of a macro]%
          {%
            Example of a macro expanding an IR addition into assembly code.
            %
            The template to match is given as argument to \cCode*{expand}, and
            the procedure to run upon expansion is given as \cCode*{expand}'s
            body%
          }
  \labelFigure{macro-example-2}
\end{figure}%
%
A \gls!{macro expander} traverses the \gls{function} under compilation and
attempts one \gls{macro} after another, typically in the order they are declared
in the \gls{machine description}.
%
Upon a \gls{match} it executes the corresponding \gls{expand procedure} and then
resumes the traversal with the next, unmatched part until the entire
\gls{function} has been expanded.
%
Consequently, \gls{matching} and \gls{selection} is combined into a single task
as the first \gls{macro} matched is the \gls{macro} to be selected.

The main benefit of \gls{macro expansion} is that it is intuitive and
straightforward to apply.
%
Because the \gls{macro expander} is implemented separately from the
\glspl{macro}, the former can be kept generic and simple while the latter can be
made as customized as needed for the \gls{target machine}.
%
This also allows the \gls{macro expander} to be void of any \glsshort{target
  machine}-specific details, thus requiring only the \glspl{macro} to be
rewritten when retargeting the \gls{compiler} to another machine.
%
To this end, the \glspl{macro} are typically written in some dedicated language
in order to simplify this task by raising the level of abstraction.


\section{Naive Macro Expansion}
\labelSection{me-naive-macro-expansion}

\subsection{Early Applications}

We will refer to \glspl{instruction selector} that directly apply the
\gls{principle} just described as \gls!{naive.me}[ \glspl{macro expander}], for
reasons that will soon become apparent.
%
In the first such implementations, the \glspl{macro} were either written by hand
-- like in the \gls{Pascal} \gls{compiler} developed by
\citeauthor{AmmannEtAl:1974}~\cite{AmmannEtAl:1974, AmmannEtAl:1977} -- or
generated automatically from a \gls{machine description}, typically written in
some dedicated language.
%
Consequently, many such languages and related tools have appeared -- and then
disappeared -- over the years (see for example \cite{Brown:1969} for an early
survey).

One such example is \gls!{Simcmp}, a \gls{macro expander} developed in 1969 by
\textcite{OrgassWaite:1969}.
%
Designed to facilitate \gls{bootstrapping},\!%
%
\footnote{%
  \Gls!{bootstrapping} is the process of writing a \gls{compiler} in the
  programming language it is intended to compile.%
}
%
\gls{Simcmp} read its input line by line, compared the line against the
\glspl{template} of the available \glspl{macro} (see \refFigure{simcmp-example}
for an example), and then executed the first macro that matched.

\begin{filecontents*}{simcmp-example-macro.c}
* = CAR.*.
    I = CDR('21)
    CDR('11) = CAR(I).
.X
\end{filecontents*}
%
\begin{filecontents*}{simcmp-example-input.c}
A = CAR B.
\end{filecontents*}
%
\begin{filecontents*}{simcmp-example-result.c}
I = CDR(38)
CDR(36) = CAR(I)
\end{filecontents*}
%
\begin{figure}
  \figureFont\figureFontSize%
  \centering%

  \subcaptionbox{%
                  A macro definition%
                  \labelFigure{sicmp-example-macro}%
                }%
                {%
                  \begin{lstpage}{40mm}%
                    \lstinputlisting{simcmp-example-macro.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  String that matches the template%
                  \labelFigure{sicmp-example-input}%
                }%
                [32mm]%
                {%
                  \begin{lstpage}{19.5mm}%
                    \lstinputlisting{simcmp-example-input.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  After macro expansion%
                  \labelFigure{sicmp-example-result}%
                }%
                [36mm]%
                {%
                  \begin{lstpage}{30mm}%
                    \lstinputlisting{simcmp-example-result.c}%
                  \end{lstpage}%
                }

  \caption[Example of macro expansion using \glsentrytext{Simcmp}]%
          {%
            Example of macro expansion using \glsentrytext{Simcmp}
            \cite{OrgassWaite:1969}%
          }
  \labelFigure{simcmp-example}
\end{figure}

Another example is the \gls{GCL}, developed by \textcite{ElsonRake:1970}, which
was used in a \gls{PL/1} \gls{compiler} for generating \gls{assembly code} from
\glspl!{AST}, which are \gls{graph}-based representations of the source code
that are always shaped like \glspl{tree}.
%
The most important feature of these \glspl{tree} is that only a syntactically
valid \gls{function} can be transformed into an~\gls{AST}, which simplifies the
task of the \gls{instruction selector}.
%
However, the basic \gls{principle} of \gls{macro expansion} remains the same.


\subsubsection{Using IR Instead of ASTs}
\labelSection{me-expression-tree}

\glsreset{IR}

Performing \gls{instruction selection} directly on the source code, either in
its textual form or on the \gls{AST}, carries the disadvantage of tightly
coupling the \gls{backend} to a particular programming language.
%
Most \gls{compiler} infrastructures therefore rely on some lower-level,
machine-independent \gls!{IR} which isolates the subsequent
\glsshort{target machine}-independent optimizations and the \gls{backend} from
the details of the programming language.
%
The \gls{IR} code is often represented as an \gls!{expression tree}, which is a
\gls{tree}-shaped \gls{data-flow graph} (see
\refFigure{expression-tree-example}).
%
\begin{filecontents*}{ir-code-example.c}
$\irTemp{t}$ = a + b
c = $\irTemp{1}$ * 2
\end{filecontents*}
%
\begin{figure}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  IR code%
                  \labelFigure{expression-tree-example-ir-code}%
                }{%
                  \begin{lstpage}{20mm}%
                    \lstinputlisting[mathescape]{ir-code-example.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree%
                  \labelFigure{expression-tree-example-tree}%
                }{%
                  \input{figures/macro-expansion/expression-tree-example}%
                }%
  \hfill%
  \mbox{}

  \caption{Example of an expression tree}
  \labelFigure{expression-tree-example}
\end{figure}
%
It is common to omit any intermediate variables from the \gls{expression tree}
and only keep those signifying the input and output values of the expression, as
shown in the example.
%
This also means that an \gls{expression tree} can only represent a set of
computations performed within the same \gls{block}, which thus may contain more
than one \gls{expression tree}.
%
Since these representations only capture data flow, the \gls{function}'s control
flow is represented separately as a \gls{control-flow graph}.

One of the first \gls{IR}-based schemes was developed by \textcite{Wilcox:1971}.
%
Implemented in a \gls{PL/C} \gls{compiler}, the \gls{AST} is first transformed
into machine-independent code consisting of \glspl!{SLM instruction}.
%
The \gls{instruction selector} then maps each \gls{SLM instruction} into one or
more target-specific \glspl{instruction} using \glspl{macro} defined in a
language called \gls!{ICL} (see \refFigure{icl-example} for an example).
%
\newcommand{\commentize}[1]{\textit{\figureFont#1}}%
\begin{filecontents*}{icl-example-macro.c}
ADDB  BR   A,ADDB1      $\commentize{If A is in a register, jump to ADDB1}$
      BR   B,ADDB2      $\commentize{If B is in a register, jump to ADDB2}$
      LGPR A            $\commentize{Generate code to load A into register}$

ADDB1 BR  B,ADDB3       $\commentize{If B is in a register, jump to ADDB3}$
      GRX A,A,B         $\commentize{Generate A+B}$
      B   ADDB4         $\commentize{Merge}$

ADDB3 GRR  AR,A,B       $\commentize{Generate A+B}$
ADDB4 FREE B            $\commentize{Release resources assigned to B}$
ADDB5 POP  1            $\commentize{Remove B descriptor from stack}$
      EXIT

ADDB2 GRI  A,B,A        $\commentize{Generate A+B}$
      FREE A            $\commentize{Release resources assigned to A}$
      SET  A,B          $\commentize{A now designates result location}$
      B    ADDB5        $\commentize{Merge}$
\end{filecontents*}
%
\begin{figure}%
  \centering%
  \begin{lstpage}{88mm}
    \lstinputlisting[mathescape]{icl-example-macro.c}%
  \end{lstpage}

  \caption[A binary addition macro in \glsentrytext{ICL}]%
          {A binary addition macro in \glsentrytext{ICL} \cite{Wilcox:1971}}
  \labelFigure{icl-example}
\end{figure}
%
In practice, these \glspl{macro} turned out to be tedious and difficult to
write.
%
Many details, such as addressing modes and data locations, had to be dealt with
manually from within the \glspl{macro}.
%
In the case of \gls{ICL}, the macro writer also had to keep track of which
variables were part of the final \gls{assembly code}, and which variables were
auxiliary and only used to aid the \gls{code generation} process.
%
In an attempt to simplify this task, \textcite{Young:1974} proposed (but never
implemented) a higher-level language called \gls!{TEL} that would abstract away
some of the implementation-oriented details.
%
The idea was to first express the \glspl{macro} as \gls{TEL}~code and then to
automatically generate the lower-level \gls{ICL}~\glspl{macro} from the
\gls{machine description}.


\subsection{Generating the Macros from a Machine Description}
\labelSection{separating-macros-and-machine-description}

As with \citeauthor{Wilcox:1971}'s design, many of the early
\gls{macro}-expanding \glspl{instruction selector} depended on \glspl{macro}
that were intricate and difficult to write.
%
In addition, many \gls{compiler} developers often incorporated \gls{register
  allocation} into these \glspl{macro}, which further exacerbated the
problem.
%
For example, if the \gls{target machine} exhibits multiple sets of
\glspl{register}, called \glspl!{register class}, and has special
\glspl{instruction} to move data from one \glsshort{register class} to another,
a record must be kept of which value reside in which \gls{register}.
%
Then, depending on the \gls{register} assignment, the \gls{instruction selector}
needs to emit the appropriate data-transfer \glspl{instruction} in addition to
the rest of the \gls{assembly code}.
%
Due to the exponential number of possible situations, the complexity that the
macro designer has to manage can be immense.


\subsubsection{Automatically Inferring the Necessary Data Transfers}

The first attempt to address this problem was made by \textcite{Miller:1971}.
%
In his master's thesis from~1971, \citeauthor{Miller:1971} introduces a
\gls{code generation} system called \gls!{Dmacs} that automatically infers the
necessary data transfers between memory and different \glspl{register class}.
%
By encapsulating this information in a separate \gls{machine description},
\gls{Dmacs} was also the first system to allow the details of the \gls{target
  machine} to be declared separately instead of being implicitly embedded into
the \glspl{macro}.

\gls{Dmacs} relies on two proprietary languages.
%
The first language, \gls!{MIML}, declares a set of procedural two-argument
commands that serves as the \gls{IR} format (see \refFigure{miml-example} for an
example).
%
The second language, \gls!{OMML}, is a declarative language used for
implementing the \glspl{macro} that will transform each \gls{MIML} command into
\gls{assembly code}.
%
\begin{filecontents*}{miml-example.c}
1:  SS    C,J
2:  IMUL  1,D
3:  IADD  2,B
4:  SS    A,I
5:  ASSG  4,3
\end{filecontents*}
%
\begin{figure}%
  \centering%
  \begin{lstpage}{26mm}%
    \lstinputlisting{miml-example.c}%
  \end{lstpage}

  \caption[Example of MIML code]%
          {%
            An example on how an arithmetic expression \cCode*{A[I] = B + C[J] *
              D} is represented using MIML commands~\cite{Miller:1971}.
            %
            The \cCode*{SS} command is used for data referencing and the
            \cCode*{ASSG} command assigns a value to a variable.
            %
            The arguments to the MIML commands are referred to either by a
            variable symbol or by line number%
          }
          \labelFigure{miml-example}
\end{figure}
%
So far this scheme is similar to the one applied by \citeauthor{Wilcox:1971}.

When adding support for a new \gls{target machine}, a macro designer first
specifies the set of available \glspl{register class} (including memory) as well
as the permissible transfer paths between these \glsplshort{register class}.
%
The macro designer then defines the \gls{OMML} \glspl{macro} by providing, for
each macro, a list of \glspl{instruction} that implements the corresponding
\gls{MIML} command on the \gls{target machine}.
%
If necessary, a sequence of \glspl{instruction} can be given to emulate the
effect of a single \gls{MIML} command.
%
Lastly, constraints are added that force the input and output data to reside in
the locations expected of the \gls{instruction}.
%
\RefFigure{omml-example} shows excerpts of an \gls{OMML} specification for an
\gls{IBM}~machine.

\begin{filecontents*}{omml-example.c}
rclass REG:r2,r3,r4,r5,r6
rclass FREG:fr0,fr2,fr4,fr6
...
rpath WORD->REG: L REG,WORD
rpath REG->WORD: ST REG,WORD
rpath FREG-WORD: LE FREG,WORD
rpath WORD->FREG: STE FREG,WORD
...
ISUB s1,s2
from REG(s1),REG(s2) emit SR s1,s2  result REG(s1)
from REG(s1),WORD(s2) emit S s1,s2  result REG(s2)

FMUL m1,m2 (commutative)
from FREG(m1),FREG(m2) emit MER m1,m2  result FREG(m1)
from FREG(m1),WORD(m2) emit ME m1,m2   result FREG(m1)
\end{filecontents*}
%
\begin{figure}%
  \centering%
  \begin{lstpage}{95mm}%
    \lstinputlisting{omml-example.c}%
  \end{lstpage}

  \caption[Example of OMML code]%
          {%
            Partial machine description for IBM-360 in OMML~\cite{Miller:1971}.
            %
            The \cCode*{rclass} command declares a register class, and the
            \cCode*{rpath} command declares a permissible transfer between a
            register class and memory (or vice versa) along with the instruction
            that implements the transfer%
          }
  \labelFigure{omml-example}
\end{figure}

\gls{Dmacs} uses this information to generate a collection of \glspl{finite
  state automaton} (or \glspl{state machine}, as they are also called) to
determine how a given set of input values can be transferred into locations that
are permissible for a given \gls{OMML} \gls{macro}.
%
Each \gls{state machine} consists of a \gls{directed.g} \gls{graph} where a
\gls{node} represents a specific configuration of \glspl{register class} and
memory, some of which are marked as permissible.
%
The edges indicate how to transition from one state to another, and are labeled
with the machine instruction that will enable the transition when executed on a
particular input value.
%
During compilation the \gls{instruction selector} consults the appropriate
\gls{state machine} as it traverses from one \gls{MIML} command to the next,
using the input values of the former to initialize the \gls{state machine}.
%
As the \gls{state machine} transitions from one state to another, the machine
instructions appearing on the edges are emitted until the \gls{state machine}
reaches a permissible state.

The work by \citeauthor{Miller:1971} was pioneering but limited:
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item \gls{Dmacs} only handled arithmetic expressions consisting of integer
    and floating-point values
  \item its addressing mode support was limited
  \item it could not model other \gls{target machine} classes such as
    stack-based architectures
\end{inlinelist}.
%
In his 1973 doctoral dissertation, \textcite{Donegan:1973} extended
\citeauthor{Miller:1971}'s ideas by proposing a new language called \gls!{CGPL}.
%
\citeauthor{Donegan:1973}'s proposal was put to the test in the 1978 master's
thesis by \textcite{Maltz:1978}, and was later extended by
\textcite{Donegan:1979}.
%
Similar techniques have also been developed by \textcite{Tirrell:1973} and
\textcite{Simoneaux:1975}.
%
\textcite{GanapathiEtAl:1982:Survey} also describe in their survey another
\gls{state machine}-based \gls{compiler} called \gls!{Ugen}, which was derived
from a virtual machine called \gls!{U-Code}~\cite{PerkinsSites:1979}.


\subsubsection{Further Improvements}

\glsunset{PCC}

In 1975, \textcite{Snyder:1975} implemented one of the first fully operational
and portable \gls{C}~\glspl{compiler}, where the \gls{target machine}-dependent
parts could be automatically generated from a \gls{machine description}.
%
The design is similar to \citeauthor{Miller:1971}'s in that the \gls{frontend}
first transforms the \gls{function} into an equivalent representation for an
abstract machine.
%
In \citeauthor{Snyder:1975}'s design this representation consists of
\glspl!{AMOP}, which are then expanded into target-specific \glspl{instruction}
via \glspl{macro}.
%
The abstract machine and \glspl{macro} are specified in a \gls{machine
  description} language which is also similar to \citeauthor{Miller:1971}'s, but
handles more complex data types, addressing modes, alignment, as well as
branching and function calls.
%
If needed, more complicated \glspl{macro} can be defined as customized
\gls{C}~functions.
%
We mention \citeauthor{Snyder:1975}'s work primarily because it was later
adapted by \textcite{Johnson:1978} in his implementation of \gls{PCC}, which we
will discuss in \refAppendix{tree-covering}.

\glsreset{PCC}

\citeauthor{Fraser:1977:Paper}~\cite{Fraser:1977:Paper, Fraser:1977:Thesis} also
recognized the need for human knowledge to guide the \gls{code generation}
process, and implemented a system with the aim of facilitating the addition of
handwritten rules when these are required.
%
First the \gls{function} is transformed into a representation based on a
programming language called \gls!{XL}, which is akin to high-level \gls{assembly
  code}.
%
For example, \gls{XL} provides primitives for array accesses and for~loops.
%
As in the cases of \citeauthor{Miller:1971} and \citeauthor{Snyder:1975}, the
\glspl{instruction} are provided via a separate description that maps directly
to a distinct \gls{XL}~primitive.
%
If some portion of the \gls{function} cannot be implemented by any of the
available \glspl{instruction}, the \gls{instruction selector} will invoke a set
of rules to rewrite the \gls{XL}~code until a solution is found.
%
For example, array accesses are broken down into simpler primitives, and the
same rule base can also be used to improve the code quality of the generated
\gls{assembly code}.
%
Since these rules are provided as a separate \gls{machine description}, they can
be customized and augmented as needed to fit a particular \gls{target machine}.

As we will see, this idea of ``massaging'' the \gls{function} until a solution
can be found has been applied, in one form or another, by many
\glspl{instruction selector} that both predate and succeed
\citeauthor{Fraser:1977:Paper}'s design.
%
Although they represent a popular approach, a significant drawback of such
schemes is that the \gls{instruction selector} may get stuck in an infinite loop
if the set of rules is incomplete for a particular \gls{target machine}, and
determining if this is the case is often far from trivial.
%
Moreover, such rules tend to be hard to reuse for other \glspl{target machine}.


\subsection{Reducing Compilation Time with Tables}

Despite their already simplistic nature, \gls{macro}-expanding
\glspl{instruction selector} can be made even more so by representing the
\mbox{1-to-1} or \mbox{1-to-$n$} mappings as sets of tables.
%
This further emphasizes the separation between the machine-independent core of
the \gls{instruction selector} from the machine-dependent mappings, as well as
allows for denser implementations that require less memory and potentially
reduce the compilation time.


\subsubsection{Instruction Selection Using Bit Strings}

In 1969 \textcite{LowryMedlock:1969} introduced one of the first table-driven
methods for \gls{code generation}.

In their implementation of the \gls!{FHC}, \citeauthor{LowryMedlock:1969}
essentially perform \gls{instruction selection} after \gls{register allocation}
depending on the status of the operands to a computation.
%
These statuses are represented as bit strings, which are then matched against
the corresponding bit strings for the \glspl{instruction} (see
\refTable{instruction-bit-strings} for examples).
%
\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \newcolumntype{L}{>{\codeFont*}l}%
  \newcolumntype{T}{>{\figureFontSize\tabhead}c}%
  \begin{tabular}{LL}
    \toprule
        \multicolumn{1}{T}{instruction}
      & \multicolumn{1}{T}{bit string}\\
    \midrule
      L  B2, D(0, BD) & XXXXXXXX00000000\\
      LH B2, D(0, B2) & 0000111100000000\\
      LR R1, R2       & 0000110100001101\\
    \bottomrule
  \end{tabular}

  \caption[Example of instruction bit strings]%
          {%
            Example of instruction bit strings~\cite{LowryMedlock:1969}.
            %
            An \cVar*{X} means that it will always match any bit in a given bit
            string%
          }
  \labelTable{instruction-bit-strings}
\end{table}
%
The bits represent restrictions applied by the \glspl{instruction}, for example
that the first operand must be fetched from memory, the second operand must
reside in a \gls{register} whose content will be erased because the result will
be placed in the same \gls{register}.

The main disadvantage of \citeauthor{LowryMedlock:1969}'s design was that the
tables could only be used for the most basic of \glspl{instruction}, and had to
be written by hand in the case of \gls{FHC}.
%
More extensive designs were later developed by \textcite{Tirrell:1973} and
\textcite{Donegan:1973}, but these also suffered from similar disadvantages of
making too many assumptions about the \gls{target machine}, thus hindering
\gls{compiler} retargetability.


\subsubsection{Expanding Macros Top-Down}

Later \textcite{KrummeAckley:1982} introduced a table-driven design which,
unlike the earlier techniques, exhaustively enumerates all valid combinations of
selectable \glspl{instruction}, schedules, and \glspl{register allocation} for a
given \gls{expression tree}.
%
Implemented in a \gls{C}~\gls{compiler} targeting \gls{DEC-10} machines, the
technique also allows code size to be factored in as an optimization goal, which
was an uncommon feature at the time.
%
\citeauthor{KrummeAckley:1982}'s \gls{backend} applies a recursive algorithm
that begins by selecting \glspl{instruction} for the \gls{root} in the
\gls{expression tree}, and then working its way down.
%
In comparison, the bottom-up techniques we have examined so far all start at the
leaves and then traverse upwards.
%
We settle with this distinction for now as we will resume and deepen the
discussion of bottom-up \versus top-down \gls{instruction selection} in
\refAppendix{tree-covering}.

Enumerating all valid combinations in code generation leads to a combinatorial
explosion, thus making it impossible to actually produce and check each and
every one of them.
%
To curb this immense complexity, \citeauthor{KrummeAckley:1982} applied a
variant of \gls{branch and bound} as \gls{search} strategy (see
\refChapter{constraint-programming} on \refPage{cp-branch-and-bound}).\!
%
\footnote{%
  In their paper, \citeauthor{KrummeAckley:1982} actually call this \gls{AB
    pruning}, which is an entirely different search strategy, but their
  description of it fits more the \gls{branch and bound} approach.
  %
  Both are well explained in~\cite{RussellNorvig:2010}.%
}
%
The problem is how to prove that a given branch in the \gls{search space} will
definitely lead to solutions that are worse than what we already have (and can
thus be skipped).
%
\citeauthor{KrummeAckley:1982} only partially tackled this problem by pruning
away branches that for sure will eventually lead to failure and thus yield no
solution whatsoever.
%
Without going into too much detail, this is done by using not just a single
\gls{instruction} table but several -- one for each so-called \emph{mode} --
which are constructed in a hierarchical manner.
%
In this context, a mode is oriented around the result of an expression, for
example whether it is to be stored in a \gls{register} or in memory.
%
Using these tables, the \gls{instruction selector} can look ahead and detect
whether the current set of already-selected \glspl{instruction} will lead to a
dead end.
%
With this as the only method of branch pruning, however, the \gls{instruction
  selector} will make many needless revisits in the \gls{search space}, and
consequently does not scale to larger \glspl{expression tree}.


\subsection{Falling Out of Fashion}

Despite the improvements we have just discussed, they still do not resolve the
main disadvantage of \gls{macro}-expanding \glspl{instruction selector} --
namely, that they can only handle \glspl{macro} that expand a single \gls{AST}
or \gls{IR}~\gls{node} at a time.
%
The limitation can be somewhat circumvented by allowing information about the
visited \glspl{node} to be forwarded from one \gls{macro} to the next, thereby
postponing \gls{assembly code} emission in the hopes that more efficient
\glspl{instruction} can be used.
%
However, if done manually -- which was often the case -- this quickly becomes an
unmanageable task for the \gls{macro} writer, in particular if backtracking
becomes necessary due to faulty decisions made in prior \gls{macro} invocations.

Thus \gls{naive.me} \glspl{macro expander} are effectively limited to supporting
only \gls{single-output.ic} \glspl{instruction}.\!%
%
\footnote{%
  This is a truth with modification.
  %
  A \gls{macro expander} can emit \gls{multi-output.ic} \glspl{instruction}, but
  only one of its output values will be retained in the \gls{assembly code}.%
}
%
This has a detrimental effect on code quality for \glspl{target machine}
exhibiting more complicated features, such as \gls{multi-output.ic}
\glspl{instruction}
%
Consequently, \glspl{instruction selector} based solely on \gls{naive.me}
\gls{macro expansion} were quickly replaced by newer, more powerful techniques
that started to appear in the late~1970s.
%
One of these we will discuss later in this appendix.


\subsubsection{%
  Rekindled Application in the First Dynamic Code Generation Systems%
}

Having fallen out of fashion, \gls{naive.me}[ly] \gls{macro}-expanding
\glspl{instruction selector} later made a brief reappearance in the first
dynamic \gls{code generation} systems that were developed in the 1980s and
1990s.
%
In such systems the \gls{function} is first compiled into \gls!{byte code},
which is a kind of target-independent \gls{machine code} that can be interpreted
by an underlying runtime environment.
%
By providing an identical environment on every \gls{target machine}, the same
\gls{byte code} can be executed on multiple systems without having to be
recompiled.

The cost of this portability is that running a \gls{function} in interpretive
mode is typically much slower than executing native \gls{machine code}.
%
This performance loss can be mitigated by incorporating a \gls{compiler} into
the runtime environment.
%
First, the \gls{byte code} is profiled as it is executed.
%
Frequently executed segments, such as inner loops, are then compiled into native
\gls{machine code}.
%
Since the code segments are compiled at runtime, this scheme is called \gls!{JIT
  compilation}, which allows performance to be increased while retaining the
benefits of the \gls{byte code}.
%
If the performance gap between running \gls{byte code} instead of native
\gls{machine code} is large, then the \gls{compiler} can afford to produce
\gls{assembly code} of low quality in order to decrease the overhead in the
runtime environment.
%
This was of great importance for the earliest dynamic runtime systems where
hardware resources were typically scarce, which made \gls{macro}-expanding
\gls{instruction selection} a reasonable option.
%
A few examples include interpreters for
\gls{Smalltalk-80}~\cite{DeutschSchiffman:1984} and
\gls{Omniware}~\cite{Adl-TabatabaiEtAl:1996} (a predecessor to \gls{Java}).
%
More examples include \gls{code generation} systems such as
\gls!{VCode}~\cite{Engler:1996}, \gls!{GBurg}~\cite{FraserProebsting:1999} (used
inside a small virtual machine), and \gls!{Gnu Lightning}~\cite{GNUlightning} (a
\gls{code generation} library inspired by \gls{VCode}).

As technology progressed, however, dynamic \gls{code generation} systems also
began to transition to more powerful techniques for \gls{instruction selection}
such as \gls{tree covering}, which will be described in
\refAppendix{tree-covering}.


\section{Improving Code Quality with Peephole Optimization}
\labelSection{me-peephole-optimization}

An early but still applied method of improving the quality of generated
\gls{assembly code} is to perform a subsequent \gls{program} optimization step
that attempts to combine and replace several \glspl{instruction} with shorter,
more efficient alternatives.
%
These routines are known as \glspl!{peephole optimizer} for reasons which will
soon become apparent.


\subsection{What Is Peephole Optimization?}

In 1965, \textcite{McKeeman:1965} advocated the use of a simple but often
neglected \gls{program} optimization procedure known as \gls!{peephole
  optimization}.
%
As a post-step to \gls{code generation}, \gls{peephole optimization} inspects a
small sequence of \glspl{instruction} in the \gls{assembly code} and attempts to
combine two or more adjacent \glspl{instruction} with a single instruction.
%
The name is thus derived from its narrow window of observation, and similar
ideas were also suggested by \textcite{LowryMedlock:1969} around the same time.
%
Doing this reduces code size and improves performance as using complex
\glspl{instruction} is often more efficient than using several simpler
\glspl{instruction} to implement the same functionality.\!%
%
\footnote{%
  This idea was applied by \textcite{ChoEtAl:2006} for reselecting instructions
  in order to improve iterative modulo schedules for \glspl{DSP}.%
}


\subsubsection{Modeling Instructions with Register Transfer Lists}
\labelSection{me-register-transfer-lists}

Since this kind of optimization is tailored for a particular \gls{target
  machine}, the earliest implementations were (and still often are) done ad hoc
and by hand.
%
For example, in 2002, \textcite{KrishnaswamyGupta:2002} wrote a \gls{peephole
  optimizer} by hand which reduces code size by replacing known \glspl{pattern}
of \gls{ARM}~code with smaller equivalents.
%
Recognizing the need for automation, \textcite{Fraser:1979} introduced in 1979
the first technique that allowed \glspl{peephole optimizer} to be generated from
a formal description.
%
The technique is also described in a longer article by
\textcite{DavidsonFraser:1980}.

Like \citeauthor{Miller:1971}, \citeauthor{Fraser:1979} described the semantics
of the \glspl{instruction} separately in a symbolic \gls{machine description}.
%
The \gls{machine description} describes the observable effects that each
\gls{instruction} has on the \gls{target machine}'s \glspl{register}.
\citeauthor{Fraser:1979} called these effects \glspl!{RT}, and each
\gls{instruction} thus has a corresponding \gls!{RTL}.
%
For example, assume that we have a three-address \cCode*{add}~\gls{instruction}
which adds an immediate value~\cVar*{imm} to the value in
\gls{register}~\cVar*{r}[s], stores the result in \gls{register}~\cVar*{r}[d],
and sets a zero flag~\cVar*{Z}.
%
For this \gls{instruction}, the corresponding \gls{RTL} would be expressed as
\begin{displaymath}
  \mFunFont{\gls{RTL}}(\text{\cCode*{add}}) =
  \left\{
    \begin{array}{r@{\;\; \leftarrow \;\;}l}
        \text{\cVar*{r}[d]}
      & \text{\cVar*{r}[s]} + \text{\cCode{imm}} \\
        \text{\cVar*{Z}}
      & \text{\cVar*{r}[s]} + \text{\cCode{imm}} = 0
    \end{array}
  \right\}\!.
\end{displaymath}

The \glspl{RTL} are then fed to a \gls{program} called \gls!{PO}, which produces
a \gls{program} optimization routine that makes two passes over the generated
\gls{assembly code}.
%
The first pass runs backwards across the \gls{assembly code} to determine the
observable effects (that is, the \gls{RTL}) of each \gls{instruction} in the
\gls{assembly code}.
%
This allows effects that have no impact on the \gls{function}'s observable
behavior to be removed.
%
For example, if the value of a \gls{status flag} is not read by any subsequent
\gls{instruction}, it is considered to be \gls!{unobservable.RT} and can thus be
ignored.
%
The second pass then checks whether the combined \glspl{RTL} of two adjacent
\glspl{instruction} are equal to that of some other \gls{instruction} (in
\gls{PO} this check is done via a series of string comparisons).
%
If such an instruction is found, the pair is replaced and the routine backs up
one \gls{instruction} in order to check the combination of the new
\gls{instruction} with the following \gls{instruction} in the \gls{assembly
  code}.
%
This way replacements can be cascaded and many \glspl{instruction} reduced into
a single equivalent, provided there exists an appropriate \gls{instruction} for
each intermediate step.

Pioneering as it was, \gls{PO} also had several limitations.
%
The main drawbacks were that it only supported combinations of two
\glspl{instruction} at a time, and that these had to be lexicographically
adjacent in the \gls{assembly code}.
%
The \glspl{instruction} were also not allowed to cross \gls{block} boundaries,
meaning that they had to belong to the same \gls{block}.
%
\textcite{DavidsonFraser:1984} later removed the limitation of lexicographical
adjacency by making use of \glspl{data-flow graph} instead of operating directly
on the \gls{assembly code}.
%
In addition, they extended the size of the \gls{instruction} window from pairs
to triples.


\subsubsection{Further Developments}

Much research has been dedicated to improving automated approaches to
\gls{peephole optimization}.
%
In 1983, \textcite{Giegerich:1983} proposed a formal design that eliminates the
need for a fixed-size \gls{instruction} window.
%
Shortly after, \textcite{Kessler:1984} introduced a method where \gls{RTL}
combinations and comparisons can be precomputed as the \gls{compiler} is built,
thus decreasing compilation time.
%
\textcite{Kessler:1986} later expanded his work to incorporate an
\mbox{$n$-size} \gls{instruction} window, similar to that of
\citeauthor{Giegerich:1983}, although at an exponential cost.

Another scheme was developed by \textcite{Massalin:1987} who implemented a
system called the \gls!{Superoptimizer}, and similar systems have subsequently
been referred to as \glspl!{superoptimizer}.
%
The \gls{Superoptimizer} accepts small \glspl{function} written in \gls{assembly
  code}, and then exhaustively combines sequences of \glspl{instruction} to find
shorter implementations that exhibit the same behavior as the original
\gls{function}.\!%
%
\footnote{%
  The same idea has also been applied by \textcite{El-KhalilKeromytis:2004} and
  \textcite{AnckaertEtAl:2005}, where the \gls{assembly code} of compiled
  \glspl{function} is modified in order to support \gls!{steganography} (the
  covert insertion of secret messages).
  %
  For example, \citeauthor{AnckaertEtAl:2005} used this technique on nine
  \glspl{function} from the \gls!{SPECint 2000} benchmark suite in order to
  embed and extract William Shakespeare's play \textit{King Lear}.%
}
%
\textcite{GranlundKenner:1992} later adapted \citeauthor{Massalin:1987}'s ideas
into a method that minimizes the number of branches.
%
Both implementations, however, were implemented by hand and
customized for a particular \gls{target machine}.
%
Moreover, neither makes any guarantees on correctness.
%
A technique for automatically generating \gls{peephole optimization}-based
\glspl{superoptimizer} was developed by \textcite{BansalAiken:2006}, where the
\gls{superoptimizer} learns to optimize short sequences of \glspl{instruction}
from a set of training \glspl{function}.
%
A couple of designs that guarantee correctness have been developed by
\citeauthor{JoshiEtAl:2002}~\cite{JoshiEtAl:2002, JoshiEtAl:2006} and
\textcite{CrickEtAl:2009}, who applied automatic theorem proving and \gls!{ASP}
(see \cite{GrassoEtAl:2013} for an overview of \gls{ASP}), respectively.
%
Recently, a similar technique based on \gls{quantifier-free
  bit-vector logic} formulas was introduced by \textcite{SrinivasanReps:2015}.


\subsection{Combining Naive Macro Expansion with Peephole Optimization}
\labelSection{macro-expansion-with-peephole-optimization}

Up to this point peephole optimizers had mainly been used to improve
already-generated \gls{assembly code} -- in other words, \emph{after}
instruction selection had been performed.
%
In 1984, however, \textcite{DavidsonFraser:1984} developed an \gls{instruction
  selection} technique that incorporates the power of \gls{peephole
  optimization} with the simplicity of \gls{macro expansion}.
%
Similar yet unsuccessful strategies had already been proposed earlier by
\textcite{AuslanderHopkins:1982} and \textcite{Harrison:1979}, but
\citeauthor{DavidsonFraser:1984} struck the right balance between \gls{compiler}
retargetability and code quality which made it a viable option for
production-quality \glspl{compiler}.
%
This scheme has hence become known as the \gls!{Davidson-Fraser approach}, and
variants of it have been used in several \glspl{compiler}, such as the
\gls!{YC}~\cite{DavidsonFraser:1982}, the
\gls!{ZephyrVPO}~system~\cite{AppelEtAl:1998}, the
\gls!{ACK}~\cite{TanenbaumEtAl:1983}, and, most famously the
\gls!{GCC}~\cite{Stallman:1988, Khedker:2012}.


\subsubsection{The Davidson-Fraser Approach}

In the \gls{Davidson-Fraser approach} the \gls{instruction selector} consists of
two parts:
%
\begin{inlinelist}[itemjoin={\ }, itemjoin*={\ and}]
  \item an \gls!{expander}
  \item a \gls!{combiner} (see \refFigure{davidson-fraser-approach})
\end{inlinelist}.
%
\begin{figure}
  \centering%
  \input{figures/macro-expansion/davidson-fraser-approach}

  \caption{Overview of the Davidson-Fraser approach}
  \labelFigure{davidson-fraser-approach}
\end{figure}
%
The task of the \gls{expander} is to transform the \gls{function} into a series
of \glspl{RTL}.
%
The transformation is done by executing simple \glspl{macro} that expand every
\gls{node} in the \gls{expression tree} (assuming the \gls{function} is
represented as such) into a corresponding \gls{RTL} that describes the effects
of that \gls{node}.
%
Unlike the previous \glspl{macro expander} we have discussed, these
\glspl{macro} do not incorporate \gls{register allocation}.
%
Instead the \gls{expander} assigns each result to a virtual storage location
called a \gls{temporary}, of which it is assumed there exists an infinite
amount.
%
A subsequent \gls{register allocator} then assigns each temporary to a
\gls{register}, potentially inserting additional code that saves some values to
memory for later retrieval when the number of available \glspl{register} is not
enough (this is called \gls!{spilling.r}).
%
After expansion, but before \gls{register allocation}, the \gls{combiner} is
run.
%
Using the same technique as that behind \gls{PO}, the \gls{combiner} tries to
improve code quality by combining several \glspl{RTL} in the \gls{function} into
a single, larger \gls{RTL} that corresponds to some \gls{instruction} on the
\gls{target machine}.
%
For this to work, both the \gls{expander} and the \gls{combiner} must at every
step adhere to a rule, called the \gls!{machine invariant}, which dictates that
every \gls{RTL} in the \gls{function} must be implementable by a single
\gls{instruction}.

By using a subsequent \gls{peephole optimizer} to combine the effects of
multiple \glspl{RTL}, the \gls{instruction selector} can effectively extend over
multiple \glspl{node} in the \gls{AST} or \gls{expression tree}, potentially
across \gls{block} boundaries.
%
The \gls{instruction} support in \citeauthor{DavidsonFraser:1984}'s design is
therefore in theory only restricted by the number of \glspl{instruction} that
the \gls{peephole optimizer} can compare at a time.
%
For example, opportunities to replace three \glspl{instruction} by a single
\gls{instruction} will be missed if the \gls{peephole optimizer} only checks
pair combinations.
%
But increasing the window size typically incurs an exponential cost in terms of
added complexity, thus making it difficult to handle complicated
\glspl{instruction} that require large \gls{instruction} windows.


\subsubsection{Further Improvements}

\textcite{FraserWendt:1988} later expanded the work by
\citeauthor{DavidsonFraser:1984}.
%
In a paper from~1988, \citeauthor{FraserWendt:1988} describe a method where the
\gls{expander} and \gls{combiner} are effectively fused together into a single
component.
%
The idea is to generate the \gls{instruction selector} in two steps.
%
The first step produces a \gls{naive.me} \gls{macro expander} that is capable of
expanding a single \gls{IR} \gls{node} at a time.
%
Unlike \citeauthor{DavidsonFraser:1984}, who implemented the \gls{expander} by
hand, \citeauthor{FraserWendt:1988} devised a design that can be automatically
generated from a \gls{machine description}.
%
The design relies on an elaborate scheme consisting of a series of switch and
goto statements, which effectively implement a \gls{state machine}.
%
Once produced, the \gls{macro expander} is executed on a carefully designed
training set.
%
Using function calls embedded into the \gls{instruction selector}, a
retargetable \gls{peephole optimizer} is executed in tandem which discovers and
gathers statistics on target-specific optimizations that can be done on the
generated \gls{assembly code}.
%
Based on these results, the beneficial optimization decisions are then selected
and incorporated directly into the \gls{macro expander}.
%
This effectively enables the \gls{macro expander} to expand multiple \gls{IR}
\glspl{node} at a time, thus removing the need for a separate \gls{peephole
  optimizer} in the final \gls{compiler}.
%
\citeauthor{FraserWendt:1988} argued that as the \gls{instruction selector} only
implements the optimization decisions that are deemed to be ``useful,'' the code
quality is improved with minimal overhead.
%
\textcite{Wendt:1990} later improved the technique by providing a more powerful
\gls{machine description} format, also based on \glspl{RTL}, which subsequently
evolved into a compact standalone language used for implementing \glspl{code
  generator} (see \textcite{Fraser:1989}).


\subsubsection{Enforcing the Machine Invariant with a Recognizer}

The \gls{Davidson-Fraser approach} was also extended by
\textcite{DiasRamsey:2006}.
%
Instead of requiring each separate \gls{RTL}-oriented optimization routine to
abide by the \gls{machine invariant}, \citeauthor{DiasRamsey:2006}'s design
employs a \gls!{recognizer} to determine whether an optimization decision
violates the aforementioned restriction (see \refFigure{dias-ramsey-approach}).
%
\begin{figure}
  \centering%
  \input{figures/macro-expansion/dias-ramsey-approach}

  \caption[Extension of the Davidson-Fraser approach]%
          {Overview of \citeauthor{DiasRamsey:2006}'s design}
  \labelFigure{dias-ramsey-approach}
\end{figure}
%
The idea is that, by doing so, the optimization routines can be simplified and
generated automatically as they no longer need to internalize the \gls{machine
  invariant}.

In a paper from~2006, \citeauthor{DiasRamsey:2006} demonstrate how the
\gls{recognizer} can be produced from a declarative \gls{machine description}
written in \gls{lambdaRTL}.
%
Originally developed by \textcite{RamseyDavidson:1998}, \gls!{lambdaRTL} is a
high-level functional language based on \gls!{ML} and raises the level of
abstraction for writing \glspl{RTL} (see \refFigure{lambda-rtl-example} for an
example).
%
\begin{filecontents*}{add-instr-lambda-rtl.c}
default attribute
  add(rd, rs1, rs2) is $r[rd] := $rs[rs1] + $r[rs2]
\end{filecontents*}
%
\begin{figure}
  \centering%
  \begin{lstpage}{9cm}%
    \centering%
    \lstinputlisting{add-instr-lambda-rtl.c}%
  \end{lstpage}

  \caption[Example of an instruction expressed in \glsentrytext{lambdaRTL}]%
          {%
            An \cCode*{add} instruction from the \glsentrytext{PowerPC}
            instruction set, specified using
            \glsentrytext{lambdaRTL}~\cite{DiasRamsey:2010}%
          }
  \labelFigure{lambda-rtl-example}
\end{figure}
%
In their paper, \citeauthor{DiasRamsey:2006} claim that \gls{lambdaRTL}-based
\glspl{machine description} are more concise and simpler to write compared to
those of many other designs, including \gls{GCC}.
%
In particular, \gls{lambdaRTL} is precise and unambiguous, which makes it
suitable for automated tool generation and verification.
%
The latter has been explored by \textcite{FernandezRamsey:1997} and
\textcite{BaileyDavidson:2003}.

The \gls{recognizer} checks whether an \gls{RTL} in the \gls{function} fulfills
the \gls{machine invariant} by performing a syntactic comparison between that
\gls{RTL} and the \glspl{RTL} of the \glspl{instruction}.
%
However, if a given \gls{RTL} in the \gls{function} has $n$~operations, and a
given \gls{lambdaRTL} description contains $m$~\glspl{instruction} whose
\gls{RTL} contains $l$~operations, then a naive implementation would take
$\mBigO(nml)$ time to check a single \gls{RTL}.
%
Instead, using techniques to be discussed in \refAppendix{tree-covering},
\citeauthor{DiasRamsey:2006} automatically generate the \gls{recognizer} as a
\gls{finite state automaton} that can compare a given \gls{RTL} against all
\glspl{RTL} in the \gls{lambdaRTL} description with a single check.


\subsubsection{``One Program to Expand Them All''}

In 2010, \citeauthor{DiasRamsey:2010}~\cite{DiasRamsey:2010, RamseyDias:2011}
introduced a scheme where the \gls{macro expander} only needs to be implemented
once per every distinct \emph{architecture family} instead of once per every
distinct \emph{\gls{instruction set}}.
%
For example, \gls{register}-based and stack-based machines are two separate
architecture families, whereas \gls{X86}, \gls{PowerPC}, and \gls{Sparc} are
three different \glspl{instruction set}.
%
In other words, if two \glspl{target machine} belong to the same architecture
family, then the same \gls{expander} can be used despite the differing details
in their \glspl{instruction set}.
%
This is useful because the correctness of the \gls{expander} only needs to be
proven once, which is a difficult and time-consuming process if it is written by
hand.

The idea is to have a predefined set of \glspl{tile} that are specific for a
particular architecture family.
%
A \gls!{tile} represents a simple operation which is required for any
\gls{target machine} belonging to that architecture family.
%
For example, stack-based machines require \glspl{tile} for push and pop
operations, which are not necessary on \gls{register}-based machines.
%
Then, instead of expanding each \gls{IR} \gls{node} in the \gls{function} into a
sequence of~\glspl{RTL}, the \gls{expander} expands it into a sequence of
\glspl{tile}.
%
Since the set of \glspl{tile} is identical for all \glspl{target machine} within
the same architecture family, the \gls{expander} only needs to be implemented
once.
%
After \gls{macro expansion} the \glspl{tile} are replaced by the
\glspl{instruction} used to implement each \gls{tile}, and the resulting
\gls{assembly code} can then be improved by the \gls{combiner}.

A remaining problem is how to find \glspl{instruction} to implement a given
\gls{tile} for a particular \gls{target machine}.
%
In the same papers, \citeauthor{DiasRamsey:2010} describe a scheme for doing
this automatically.
%
By expressing both the \glspl{tile} and the \glspl{instruction} as
\gls{lambdaRTL}, \citeauthor{DiasRamsey:2010} developed a technique where the
\glspl{RTL} of the \glspl{instruction} are combined such that the effects equal
that of a \gls{tile}.
%
In broad outline, the algorithm maintains a pool of \glspl{RTL} which initially
contains those of the \glspl{instruction} found in the \gls{machine
  description}.
%
Using algebraic laws and combining existing \glspl{RTL} to produce new
\glspl{RTL}, the pool is grown iteratively until either all \glspl{tile} have
been implemented, or some termination criterion is reached.
%
The latter is necessary, as \citeauthor{DiasRamsey:2010} proved that the general
problem of finding implementations for arbitrary \glspl{tile} is undecidable.

Although the primary aim of \citeauthor{DiasRamsey:2010}'s design is to
facilitate \gls{compiler} retargetability, some experiments suggest that it
potentially also yields better code quality than the original
\gls{Davidson-Fraser approach}.
%
When a prototype was compared against the default \gls{instruction selector} in
\gls{GCC}, the results favored the former.
%
However, this was seen only when all target-independent optimizations were
disabled; when they were reactivated, \gls{GCC} still produced better results.


\subsection{Running Peephole Optimization Before Instruction Selection}

In the techniques just discussed, the \gls{peephole optimizer} runs after
\gls{code generation}.
%
But in a scheme developed in 1989 by \textcite{GeninEtAl:1989}, a similar
routine is executed \emph{before} \gls{code generation}.
%
Targeting \glspl{DSP}, their \gls{compiler} first transforms the \gls{function}
into an \gls!{ISFG}, and then executes a routine -- \citeauthor{GeninEtAl:1989}
called it a \gls!{pattern matcher} -- which attempts to find several low-level
operations in the \gls{ISFG} that can be merged into single \glspl{node}.\!%
%
\footnote{%
  The paper is not clear on how this is done exactly, but presumably
  \citeauthor{GeninEtAl:1989} implemented the routine as a handwritten
  \gls{peephole optimizer} since the intermediate format is fixed and does not
  change from one \gls{target machine} to another.%
}
%
\Gls{code generation} is then done following the conventional \gls{macro
  expansion} approach.
%
For each \gls{node} the \gls{instruction selector} invokes a rule along with the
information about the current context.
%
The invoked rule produces the \gls{assembly code} appropriate for the given
context, and can also insert new \glspl{node} to offload decisions that are
deemed better handled by the rules corresponding to the inserted \glspl{node}.

According to \citeauthor{GeninEtAl:1989}, experiments show that their
\gls{compiler} generated \gls{assembly code} that was five to \num{50}~times
faster than that produced by other, contemporary \gls{DSP} \glspl{compiler}, and
comparable with manually optimized \gls{assembly code}.
%
A disadvantage of this design is that it is limited to \glspl{function} where
prior knowledge about the application area -- in this case digital signal
processing -- can be encoded into specific optimization routines, which most
likely has to be done manually.


\subsection{Interactive Code Generation}

The aforementioned techniques yield \glspl{peephole optimizer} which are static
once they have been generated, meaning they will only recognize and optimize
\gls{assembly code} for a fixed set of \glspl{pattern}.
%
A method to overcome this issue has been designed by
\textcite{KulkarniEtAl:2006}, which is also the first and only one to my
knowledge.

In a paper from~2006, \citeauthor{KulkarniEtAl:2006} describe a \gls{compiler}
system called \gls!{Vista}, which is an interactive compilation environment
where the user is given greater control over the \gls{compiler}.
%
Among other things, the user can alter \glspl{RTL} derived from the
\gls{function}'s source code and add new customized \gls{peephole optimization}
\glspl{pattern}.
%
Hence optimization privileges which normally are limited to low-level
\gls{assembly code} programmers are also granted to higher-level programming
language users.
%
In addition, \citeauthor{KulkarniEtAl:2006} employed genetic algorithms -- these
will be explained in \refAppendix{tree-covering} -- in an attempt to
automatically derive a combination of user-provided optimization guidelines to
improve the code quality of a particular \gls{function}.
%
Experiments show that this scheme reduced code size on average by
\SI{4}{\percent} and up to \SI{12}{\percent} for a selected set of
\glspl{function}.


\section{Limitations of Macro Expansion}
\labelSection{me-limitations}

Because \gls{macro}-expanding \glspl{instruction selector} only visit and
execute \glspl{macro} one \gls{IR} \gls{node} at a time, they require a
\mbox{1-to-1} or \mbox{1-to-$n$} mapping between the \gls{IR} \glspl{node} and
the \glspl{instruction} in order to generate efficient \gls{assembly code}.
%
The limitation can be mitigated by incorporating additional logic and
bookkeeping into the \glspl{macro}, but this quickly becomes an unmanageable
task for the \gls{macro} writer if done manually.
%
Consequently, the code quality yielded by such techniques will typically be low.
%
Moreover, as \glspl{instruction} are often emitted one at a time, it also
becomes difficult to make use of \glspl{instruction} that can have unintended
effects on other \glspl{instruction}.

The \gls{Davidson-Fraser approach} is therefore a more robust approach, where
the \gls{instruction selector} is augmented with a \gls{peephole optimizer}.
%
\Gls{peephole optimization} is inherently only limited by the size of its window
of observation and can in theory support \glspl{instruction} of arbitrary
complexity.
%
Because of this versatility, the \gls{Davidson-Fraser approach} remains one of
the most powerful \gls{instruction selection} techniques to date.
%
For example, a variant is still applied in \gls{GCC} as of version~\mbox{4.8.2}.


\section{Summary}
\labelSection{me-summary}

In this appendix we have discussed techniques and designs based on a
\gls{principle} known as \gls{macro expansion}, which was the first approach to
perform \gls{instruction selection}.
%
The idea behind the \gls{principle} is to expand the \glspl{node} in the
\gls{AST} or \gls{IR} code into one or more target-specific \glspl{instruction}.
%
The expansion is done via \gls{template} matching and \gls{macro} invocation,
which yields \glspl{instruction selector} that are resource-effective and
straightforward to implement.
%
Early techniques, however, applied this principle \gls{naive.me}[ly], yielding
poor code quality.
%
Later designs, based on the \gls{Davidson-Fraser approach}, mitigate this
problem by combining \gls{instruction selection} with \gls{peephole
  optimization}, and such techniques are still popular today.

In \refAppendix{tree-covering} we will explore another \gls{principle} of
\gls{instruction selection}, which solves the problem of implementing several
\gls{AST} or \gls{IR} \glspl{node} using a single \gls{instruction} in a more
direct fashion.
